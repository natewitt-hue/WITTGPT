import os
import json
import argparse
import pickle
import numpy as np
import faiss
from sentence_transformers import SentenceTransformer

# Configuration
DB_DIR = "faiss_lore_db"
INDEX_FILE = os.path.join(DB_DIR, "lore_index.faiss")
META_FILE = os.path.join(DB_DIR, "lore_metadata.pkl")
MODEL_NAME = "all-MiniLM-L6-v2"

MUST_INDEX_KEYWORDS = ["trade", "beef", "drama", "witt", "diddy", "cheese", "commish", "scam", "robbed"]

def _load_model():
    return SentenceTransformer(MODEL_NAME)

def is_lore_query(text):
    keywords = ["history", "beef", "drama", "said", "remember", "trade reaction", "lore"]
    text_lower = text.lower()
    return any(kw in text_lower for kw in keywords)

def build_lore_context(query_text, top_k=5):
    if not os.path.exists(INDEX_FILE) or not os.path.exists(META_FILE):
        return "Lore database not built."
    
    model = _load_model()
    index = faiss.read_index(INDEX_FILE)
    with open(META_FILE, "rb") as f:
        metadata = pickle.load(f)
        
    query_vector = model.encode([query_text], convert_to_numpy=True)
    distances, indices = index.search(query_vector, top_k)
    
    results = []
    for idx in indices[0]:
        if idx != -1 and idx < len(metadata):
            results.append(metadata[idx]["formatted_text"])
            
    return "\n".join(results)

def ingest(directory):
    if not os.path.exists(directory):
        print(f"ERROR: Path does not exist: {directory}")
        return

    print(f"Initializing FAISS ingestion from {directory}...")
    model = _load_model()
    os.makedirs(DB_DIR, exist_ok=True)
    
    documents = []
    metadata = []
    author_counts = {}
    processed = 0
    inserted = 0
    
    files = [os.path.join(directory, f) for f in os.listdir(directory) if f.endswith('.json') or f.endswith('.jsonl')]
    
    for i, filepath in enumerate(files):
        print(f"Processing file {i+1}/{len(files)}: {os.path.basename(filepath)}")
        try:
            with open(filepath, "r", encoding="utf-8") as f:
                data = json.load(f)
                
            messages = data if isinstance(data, list) else data.get("messages", [])
            
            for msg in messages:
                processed += 1
                content = msg.get("content")
                if not content or not isinstance(content, str):
                    continue
                
                content = content.strip()
                if not content:
                    continue
                    
                author_obj = msg.get("author", {})
                author_name = author_obj.get("nickname") or author_obj.get("name") or "Unknown"
                timestamp = msg.get("timestamp", "")[:10] 
                
                is_high_signal = any(kw in content.lower() for kw in MUST_INDEX_KEYWORDS)
                
                if not is_high_signal:
                    if len(content) < 30:
                        continue
                    if author_counts.get(author_name, 0) > 3000:
                        continue
                        
                formatted_text = f"[{timestamp}] {author_name}: {content}"
                documents.append(formatted_text)
                metadata.append({
                    "author": author_name,
                    "timestamp": timestamp,
                    "formatted_text": formatted_text
                })
                
                author_counts[author_name] = author_counts.get(author_name, 0) + 1
                inserted += 1
                
        except Exception as e:
            print(f"Error reading {filepath}: {e}")
            
    if not documents:
        print("No valid messages found to index.")
        return
        
    print(f"Encoding {len(documents)} messages... This may take a few minutes.")
    embeddings = model.encode(documents, convert_to_numpy=True, show_progress_bar=True)
    
    print("Building FAISS index...")
    dimension = embeddings.shape[1]
    index = faiss.IndexFlatL2(dimension)
    index.add(embeddings)
    
    faiss.write_index(index, INDEX_FILE)
    with open(META_FILE, "wb") as f:
        pickle.dump(metadata, f)
        
    print(f"Success! Indexed {inserted} messages out of {processed} processed.")

def stats():
    if not os.path.exists(INDEX_FILE):
        print("Database not built.")
        return
    index = faiss.read_index(INDEX_FILE)
    with open(META_FILE, "rb") as f:
        metadata = pickle.load(f)
    print(f"FAISS Index contains {index.ntotal} vectors.")
    print(f"Metadata file contains {len(metadata)} entries.")

def test(query):
    print(f"Testing Query: '{query}'")
    res = build_lore_context(query, top_k=5)
    print(res)

    def add_single_message(author, content):
        """Adds a single new message to the FAISS index and metadata."""
    if not os.path.exists(INDEX_FILE) or not os.path.exists(META_FILE):
        return

    from datetime import datetime
    timestamp = datetime.now().strftime("%Y-%m-%d")
    formatted_text = f"[{timestamp}] {author}: {content}"

    # 1. Load existing data
    model = _load_model()
    index = faiss.read_index(INDEX_FILE)
    with open(META_FILE, "rb") as f:
        metadata = pickle.load(f)

    # 2. Embed the new message
    new_vector = model.encode([formatted_text], convert_to_numpy=True)

    # 3. Update index and metadata
    index.add(new_vector)
    metadata.append({
        "author": author,
        "timestamp": timestamp,
        "formatted_text": formatted_text
    })

    # 4. Save back to disk
    faiss.write_index(index, INDEX_FILE)
    with open(META_FILE, "wb") as f:
        pickle.dump(metadata, f)
    
    print(f"[LoreRAG] Live-indexed message from {author}")
def collection_stats():
    """Returns a dictionary with the count of indexed messages for the bot to display."""
    if not os.path.exists(INDEX_FILE):
        return {"count": 0}
    try:
        index = faiss.read_index(INDEX_FILE)
        return {"count": index.ntotal}
    except:
        return {"count": 0}

def init():
    """Initializes the lore system for the bot."""
    if not os.path.exists(INDEX_FILE) or not os.path.exists(META_FILE):
        print(f"[LoreRAG] DB files not found at {DB_DIR}")
        return False
    try:
        faiss.read_index(INDEX_FILE)
        print(f"[LoreRAG] FAISS Index loaded successfully.")
        return True
    except Exception as e:
        print(f"[LoreRAG] Initialization failed: {e}")
        return False

def add_single_message(author, content):
    """Adds a single new message to the FAISS index and metadata."""
    if not os.path.exists(INDEX_FILE) or not os.path.exists(META_FILE):
        return

    from datetime import datetime
    timestamp = datetime.now().strftime("%Y-%m-%d")
    formatted_text = f"[{timestamp}] {author}: {content}"

    # 1. Load existing data
    model = _load_model()
    index = faiss.read_index(INDEX_FILE)
    with open(META_FILE, "rb") as f:
        metadata = pickle.load(f)

    # 2. Embed the new message
    new_vector = model.encode([formatted_text], convert_to_numpy=True)

    # 3. Update index and metadata
    index.add(new_vector)
    metadata.append({
        "author": author,
        "timestamp": timestamp,
        "formatted_text": formatted_text
    })

    # 4. Save back to disk
    faiss.write_index(index, INDEX_FILE)
    with open(META_FILE, "wb") as f:
        pickle.dump(metadata, f)
    
    print(f"[LoreRAG] Live-indexed message from {author}")

if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--ingest", type=str, help="Path to Discord JSON directory")
    parser.add_argument("--stats", action="store_true", help="Print DB stats")
    parser.add_argument("--test", type=str, help="Test query")
    args = parser.parse_args()
    
    if args.ingest:
        ingest(args.ingest)
    elif args.stats:
        stats()
    elif args.test:
        test(args.test)